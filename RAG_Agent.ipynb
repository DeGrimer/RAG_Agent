{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    AutoTokenizer,\n",
    "    TokenClassificationPipeline,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from transformers.pipelines import AggregationStrategy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from typing_extensions import TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1168, which is longer than the specified 500\n",
      "Created a chunk of size 970, which is longer than the specified 500\n",
      "Created a chunk of size 862, which is longer than the specified 500\n",
      "Created a chunk of size 533, which is longer than the specified 500\n",
      "Created a chunk of size 542, which is longer than the specified 500\n",
      "Created a chunk of size 581, which is longer than the specified 500\n",
      "Created a chunk of size 552, which is longer than the specified 500\n",
      "Created a chunk of size 542, which is longer than the specified 500\n",
      "Created a chunk of size 742, which is longer than the specified 500\n",
      "Created a chunk of size 597, which is longer than the specified 500\n",
      "Created a chunk of size 731, which is longer than the specified 500\n",
      "Created a chunk of size 567, which is longer than the specified 500\n",
      "Created a chunk of size 632, which is longer than the specified 500\n",
      "Created a chunk of size 602, which is longer than the specified 500\n"
     ]
    }
   ],
   "source": [
    "raw_documents = TextLoader('text.txt').load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "documents = text_splitter.split_documents(raw_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anacondaApp\\envs\\Mlenv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "# Define keyphrase extraction pipeline\n",
    "class KeyphraseExtractionPipeline(TokenClassificationPipeline):\n",
    "    def __init__(self, model, *args, **kwargs):\n",
    "        super().__init__(\n",
    "            model=AutoModelForTokenClassification.from_pretrained(model),\n",
    "            tokenizer=AutoTokenizer.from_pretrained(model),\n",
    "            *args,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def postprocess(self, all_outputs):\n",
    "        results = super().postprocess(\n",
    "            all_outputs=all_outputs,\n",
    "            aggregation_strategy=AggregationStrategy.FIRST,\n",
    "        )\n",
    "        return np.unique([result.get(\"word\").strip() for result in results])\n",
    "model_name = \"ml6team/keyphrase-extraction-distilbert-inspec\"\n",
    "extractor = KeyphraseExtractionPipeline(model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anacondaApp\\envs\\Mlenv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "d:\\anacondaApp\\envs\\Mlenv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "biencoder = HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "retriever = Chroma.from_documents(documents, biencoder).as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f8e26cd9f0410da3123fcc2db2ae19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting the `device` argument to None from 0 to avoid the error caused by attempting to move the model that was already loaded on the GPU using the Accelerate module to the same or another device.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t-bank-ai/T-lite-instruct-0.1\")\n",
    "terminators = [\n",
    "            tokenizer.eos_token_id,\n",
    "            tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "        ]\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=\"float16\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"t-bank-ai/T-lite-instruct-0.1\",\n",
    "    device=0,\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs=dict(\n",
    "        max_new_tokens=512,\n",
    "        return_full_text=False,\n",
    "        top_k=50,\n",
    "        do_sample = True,\n",
    "        temperature=0.6,\n",
    "        eos_token_id = terminators\n",
    "    ),\n",
    "    model_kwargs={\"quantization_config\": quantization_config},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keyword(docs):\n",
    "    list_doc = []\n",
    "    i=0\n",
    "    for doc in docs:\n",
    "        keywords = extractor(doc.page_content)\n",
    "        klist = ''.join(keywords)\n",
    "        list_doc.append(klist)\n",
    "        i += 1\n",
    "    return json.dumps(list_doc, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Who is particularly prone to Alzheimer's symptoms?\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rephrase_rag_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are a helper who rephrases the question based on the provided keyword.\n",
    "\n",
    "    Return the question a JSON with a single key 'question' and no premable or explaination. .\n",
    "\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    My goal is to rephrase the question, adding keywords from the context to the question. The most suitable documents are higher in the list, use them\\n\n",
    "    Document: {context} \\n\\n\n",
    "    Question: {question} \\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\",\"context\"],\n",
    ")\n",
    "rephrase_rag_chain =  {\"context\": retriever | extract_keyword, \"question\": RunnablePassthrough()} | rephrase_rag_prompt | llm | JsonOutputParser()\n",
    "rephrase_rag_chain.invoke(\"Who can get Alzheimer's?\")['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    list_doc = []\n",
    "    i=0\n",
    "    for doc in docs:\n",
    "        curDoc = {\n",
    "            'id':i,\n",
    "            'content':doc.page_content\n",
    "        }\n",
    "        list_doc.append(curDoc)\n",
    "        i += 1\n",
    "    return json.dumps(list_doc, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_ans_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are a QA assistant.\n",
    "    Given the context information and not prior knowledge, provide a well-reasoned and informative response to the query. Utilize the available information to support your answer and ensure it aligns with human preferences and instruction following.\\n\n",
    "    The answer should be short and relevant to the user's question.\n",
    "    CONTEXT:\\n\n",
    "    {context}\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question} \\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\",\"context\"],\n",
    ")\n",
    "generate_rag_chain =  {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()} | generate_ans_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    initial_question : str\n",
    "    num_steps : int\n",
    "    new_question : str\n",
    "    final_out: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rephrase_question(state):\n",
    "    print(\"----REPHRASE BASED ON CONTEXT\")\n",
    "    question = state['initial_question']\n",
    "    num_steps = state['num_steps']\n",
    "    num_steps += 1\n",
    "    new_questions = [question]\n",
    "    model_embed = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    for i in range(4):\n",
    "        new_questions.append(rephrase_rag_chain.invoke(question)['question'])\n",
    "    sentence = model_embed.encode(new_questions)\n",
    "    score = model_embed.similarity(sentence, sentence)[0,1:]\n",
    "    similarities = torch.argmax(score).item()\n",
    "    print(score)\n",
    "    print(similarities)\n",
    "    print(new_questions)\n",
    "    return {'new_question': new_questions[similarities+1], 'num_steps': num_steps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ans(state):\n",
    "    print(\"---GENERATE ANS WITH NEW QUESTION\")\n",
    "    source_quest = state['initial_question']\n",
    "    question = state['new_question']\n",
    "    num_steps = state['num_steps']\n",
    "    num_steps += 1\n",
    "    final_out = generate_rag_chain.invoke(question)\n",
    "    return {\"final_out\":final_out,\"num_steps\":num_steps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_printer(state):\n",
    "    \"\"\"print the state\"\"\"\n",
    "    print(\"---STATE PRINTER---\")\n",
    "    print(f\"Initial question: {state['initial_question']} \\n\" )\n",
    "    print(f\"num_steps: {state['num_steps']} \\n\")\n",
    "    print(f\"new_question: {state['new_question']} \\n\" )\n",
    "    print(f\"final_out: {state['final_out']} \\n\" )\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(GraphState)\n",
    "workflow.add_node('rephrase_question',rephrase_question)\n",
    "workflow.add_node('generate_ans',generate_ans)\n",
    "workflow.add_node('state_printer',state_printer)\n",
    "\n",
    "workflow.set_entry_point('rephrase_question')\n",
    "workflow.add_edge('rephrase_question', 'generate_ans')\n",
    "workflow.add_edge('generate_ans', 'state_printer')\n",
    "workflow.add_edge(\"state_printer\", END)\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----REPHRASE BASED ON CONTEXT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anacondaApp\\envs\\Mlenv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7156, 0.7695, 0.7625, 0.4476])\n",
      "1\n",
      "[\"Who can get Alzheimer's?\", \"What populations are at higher risk for developing Alzheimer's disease?\", \"Who among individuals with aging-related symptoms might be at higher risk for developing Alzheimer's?\", \"Who is more prone to develop Alzheimer's disease?\", 'What religious orders receive grants for studies in aging-related disorders?']\n",
      "---GENERATE ANS WITH NEW QUESTION\n",
      "---STATE PRINTER---\n",
      "Initial question: Who can get Alzheimer's? \n",
      "\n",
      "num_steps: 2 \n",
      "\n",
      "new_question: Who among individuals with aging-related symptoms might be at higher risk for developing Alzheimer's? \n",
      "\n",
      "final_out: \n",
      "    According to the study findings, females with high-risk genetic variants for Alzheimer's disease (AD) are at a higher risk of disease progression due to a harmful inflammatory response in the brain's immune cells. This heightened risk is influenced by immune pathways like cGAS-STING. The research emphasizes the importance of considering sex differences in Alzheimer's research and treatment, as it may require specific therapeutic approaches tailored to men and women. \n",
      "\n",
      "\n",
      "    According to the study findings, females with high-risk genetic variants for Alzheimer's disease (AD) are at a higher risk of disease progression due to a harmful inflammatory response in the brain's immune cells. This heightened risk is influenced by immune pathways like cGAS-STING. The research emphasizes the importance of considering sex differences in Alzheimer's research and treatment, as it may require specific therapeutic approaches tailored to men and women.\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"initial_question\": \"Who can get Alzheimer's?\", \"num_steps\":0}\n",
    "output = app.invoke(inputs)\n",
    "\n",
    "print(output['final_out'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
